#![allow(non_local_definitions)]

use featureduck_core::{EntityKey, FeatureRow, FeatureValue, StorageConnector};
use featureduck_core::validation::{FeatureSchema, TypeConstraint, ValueConstraint};
use featureduck_delta::{DeltaStorageConnector, DuckDBEngineConfig};
use pyo3::exceptions::{PyRuntimeError, PyValueError};
use pyo3::prelude::*;
use pyo3::types::{PyDict, PyList};
use std::collections::HashMap;

// NOTE: delta_connector module removed - PyDeltaConnector.materialize_from_plan() was deprecated
// in favor of execute_and_write() which uses SQLFrame-generated SQL directly

// NOTE: compile_to_duckdb_sql() removed - LogicalPlan deprecated in favor of SQLFrame API
// Production SDK uses execute_and_write() with raw SQL from SQLFrame

/// Execute SQL and write results to Delta Lake
///
/// This is the bridge between SQLFrame (Python) and FeatureDuck's Rust core.
///
/// Args:
///     sql: SQL query to execute (generated by SQLFrame)
///     output_path: Delta Lake output path
///     feature_view_name: Name of the feature view
///     entity_columns: List of entity column names (e.g., ["user_id"])
///     mode: Write mode ("overwrite" or "append")
///     schema: Optional validation schema dict (for validating features before write)
///     skip_validation: If true, skip validation even if schema is provided (faster materialization)
///
/// Returns:
///     Dict with execution results
///
/// Example:
///     # Without validation (fastest)
///     result = execute_and_write(sql, path, "features", ["user_id"], skip_validation=True)
///
///     # With validation
///     schema = {"name": "features", "type_constraints": {"age": "Int"}}
///     result = execute_and_write(sql, path, "features", ["user_id"], schema=schema)
#[pyfunction]
#[pyo3(signature = (sql, output_path, feature_view_name, entity_columns, mode = "overwrite", schema = None, skip_validation = false))]
#[allow(clippy::too_many_arguments)]
fn execute_and_write(
    py: Python,
    sql: String,
    output_path: String,
    feature_view_name: String,
    entity_columns: Vec<String>,
    mode: &str,
    schema: Option<&PyDict>,
    skip_validation: bool,
) -> PyResult<PyObject> {
    // Parse schema while we have GIL (before allow_threads)
    let parsed_schema: Option<FeatureSchema> = if !skip_validation {
        if let Some(schema_dict) = schema {
            Some(parse_schema_from_dict(schema_dict)?)
        } else {
            None
        }
    } else {
        None  // skip_validation=true means no validation
    };

    // Release GIL and run async code
    py.allow_threads(|| {
        // Create tokio runtime
        let runtime = tokio::runtime::Runtime::new()
            .map_err(|e| PyRuntimeError::new_err(format!("Failed to create runtime: {}", e)))?;

        runtime.block_on(async {
            execute_and_write_impl(sql, output_path, feature_view_name, entity_columns, mode, parsed_schema).await
        })
    })
    .and_then(|result| {
        // Convert Rust HashMap to Python dict
        Python::with_gil(|py| {
            let dict = PyDict::new(py);
            dict.set_item(
                "status",
                result.get("status").unwrap_or(&"unknown".to_string()),
            )?;
            dict.set_item(
                "rows_written",
                result
                    .get("rows_written")
                    .and_then(|s| s.parse::<i64>().ok())
                    .unwrap_or(0),
            )?;
            dict.set_item(
                "output_path",
                result.get("output_path").unwrap_or(&"".to_string()),
            )?;
            if let Some(error) = result.get("error") {
                dict.set_item("error", error)?;
            }
            Ok(dict.into())
        })
    })
}

async fn execute_and_write_impl(
    sql: String,
    output_path: String,
    feature_view_name: String,
    entity_columns: Vec<String>,
    mode: &str,
    validation_schema: Option<FeatureSchema>,
) -> PyResult<HashMap<String, String>> {
    use arrow::array::{
        Array, BooleanArray, Decimal128Array, Float64Array, Int32Array, Int64Array, StringArray,
    };
    use arrow::datatypes::DataType;

    let mut result = HashMap::new();

    // 1. Create DuckDB connection (in-memory for SQL execution)
    let duckdb = duckdb::Connection::open_in_memory()
        .map_err(|e| PyRuntimeError::new_err(format!("Failed to open DuckDB: {}", e)))?;

    // 2. Prepare SQL statement
    let mut stmt = duckdb
        .prepare(&sql)
        .map_err(|e| PyRuntimeError::new_err(format!("Failed to prepare SQL: {}", e)))?;

    // 3. Execute query and get Arrow batches
    let arrow_result = stmt
        .query_arrow([])
        .map_err(|e| PyRuntimeError::new_err(format!("Failed to execute SQL: {}", e)))?;

    // 4. Get schema from Arrow result
    let schema = arrow_result.get_schema();
    let column_names: Vec<String> = schema.fields().iter().map(|f| f.name().clone()).collect();

    // 5. FAST PATH: Schema-level validation (O(columns), done ONCE)
    // This catches forbidden features, type mismatches, and missing required features
    // BEFORE we iterate through potentially millions of rows
    let needs_row_validation = if let Some(ref validation_schema) = validation_schema {
        // Validate schema (fails fast for PII, type mismatches, etc.)
        validate_arrow_schema_fast(
            validation_schema,
            &column_names,
            schema.fields(),
            &entity_columns,
        )?;

        // Only need row-level validation if there are value constraints (ranges, enums, etc.)
        validation_schema.needs_row_validation()
    } else {
        false
    };

    // 6. Convert Arrow RecordBatches to FeatureRows
    let mut feature_rows = Vec::new();
    let timestamp = chrono::Utc::now();

    // Process each RecordBatch from the Arrow iterator
    // NOTE: All .unwrap() calls in this section are SAFE because:
    // - We match on column.data_type() before downcasting
    // - Arrow's type system guarantees correctness
    // - An unwrap failure would indicate a DuckDB bug, not our code
    for batch in arrow_result {
        let num_rows = batch.num_rows();

        // Process each row in the batch
        for row_idx in 0..num_rows {
            // Extract entity keys
            let mut entities = Vec::new();
            for entity_col in &entity_columns {
                if let Some(col_idx) = column_names.iter().position(|c| c == entity_col) {
                    let column = batch.column(col_idx);

                    // Convert entity value to string based on Arrow type
                    let entity_value = match column.data_type() {
                        DataType::Utf8 => {
                            let array = column.as_any().downcast_ref::<StringArray>().unwrap();
                            array.value(row_idx).to_string()
                        }
                        DataType::Int32 => {
                            let array = column.as_any().downcast_ref::<Int32Array>().unwrap();
                            array.value(row_idx).to_string()
                        }
                        DataType::Int64 => {
                            let array = column.as_any().downcast_ref::<Int64Array>().unwrap();
                            array.value(row_idx).to_string()
                        }
                        DataType::Float64 => {
                            let array = column.as_any().downcast_ref::<Float64Array>().unwrap();
                            array.value(row_idx).to_string()
                        }
                        DataType::Boolean => {
                            let array = column.as_any().downcast_ref::<BooleanArray>().unwrap();
                            array.value(row_idx).to_string()
                        }
                        dt => {
                            return Err(PyRuntimeError::new_err(format!(
                                "Unsupported entity type for column '{}': {:?}",
                                entity_col, dt
                            )));
                        }
                    };

                    entities.push(EntityKey::new(entity_col.clone(), entity_value));
                }
            }

            // Extract features from remaining columns (skip entity columns)
            let mut features = HashMap::new();
            for (col_idx, col_name) in column_names.iter().enumerate() {
                // Skip entity columns
                if entity_columns.contains(col_name) {
                    continue;
                }

                let column = batch.column(col_idx);

                // Convert Arrow value to FeatureValue based on type
                let feature_value = if column.is_null(row_idx) {
                    FeatureValue::Null
                } else {
                    match column.data_type() {
                        DataType::Int32 => {
                            let array = column.as_any().downcast_ref::<Int32Array>().unwrap();
                            FeatureValue::Int(array.value(row_idx) as i64)
                        }
                        DataType::Int64 => {
                            let array = column.as_any().downcast_ref::<Int64Array>().unwrap();
                            FeatureValue::Int(array.value(row_idx))
                        }
                        DataType::Float64 => {
                            let array = column.as_any().downcast_ref::<Float64Array>().unwrap();
                            FeatureValue::Float(array.value(row_idx))
                        }
                        DataType::Utf8 => {
                            let array = column.as_any().downcast_ref::<StringArray>().unwrap();
                            FeatureValue::String(array.value(row_idx).to_string())
                        }
                        DataType::Boolean => {
                            let array = column.as_any().downcast_ref::<BooleanArray>().unwrap();
                            FeatureValue::Bool(array.value(row_idx))
                        }
                        DataType::Decimal128(_, scale) => {
                            // Convert Decimal128 to Float64 for now
                            let array = column.as_any().downcast_ref::<Decimal128Array>().unwrap();
                            let decimal_value = array.value(row_idx);
                            let divisor = 10_i128.pow(*scale as u32);
                            let float_value = decimal_value as f64 / divisor as f64;
                            FeatureValue::Float(float_value)
                        }
                        dt => {
                            return Err(PyRuntimeError::new_err(format!(
                                "Unsupported feature type for column '{}': {:?}",
                                col_name, dt
                            )));
                        }
                    }
                };

                features.insert(col_name.clone(), feature_value);
            }

            feature_rows.push(FeatureRow {
                entities,
                features,
                timestamp,
            });
        }
    }

    let row_count = feature_rows.len();

    // 7. ROW-LEVEL validation (only for value constraints like ranges, enums)
    // Schema-level checks (forbidden, required, types) were already done in step 5
    // This only runs if schema has value_constraints that need actual data
    if needs_row_validation {
        if let Some(ref schema) = validation_schema {
            let mut validation_errors = Vec::new();
            for (idx, row) in feature_rows.iter().enumerate() {
                // Only check value constraints, schema-level already passed
                if let Err(e) = schema.validate(row) {
                    validation_errors.push(format!("Row {}: {}", idx, e));
                    // Stop after first 10 errors to avoid overwhelming output
                    if validation_errors.len() >= 10 {
                        validation_errors.push("... (truncated, more errors exist)".to_string());
                        break;
                    }
                }
            }
            if !validation_errors.is_empty() {
                return Err(PyRuntimeError::new_err(format!(
                    "Validation failed for {} row(s):\n{}",
                    validation_errors.len().min(10),
                    validation_errors.join("\n")
                )));
            }
        }
    }

    // 8. Write to Delta Lake
    let engine_config = DuckDBEngineConfig::default();
    let connector = DeltaStorageConnector::new(&output_path, engine_config)
        .await
        .map_err(|e| PyRuntimeError::new_err(format!("Failed to create connector: {}", e)))?;

    // Write based on mode
    match mode {
        "overwrite" => {
            connector
                .write_features(&feature_view_name, feature_rows)
                .await
                .map_err(|e| PyRuntimeError::new_err(format!("Failed to write features: {}", e)))?;
        }
        "append" => {
            connector
                .write_features(&feature_view_name, feature_rows)
                .await
                .map_err(|e| {
                    PyRuntimeError::new_err(format!("Failed to append features: {}", e))
                })?;
        }
        _ => {
            return Err(PyValueError::new_err(format!(
                "Unsupported write mode: {}. Use 'overwrite' or 'append'",
                mode
            )));
        }
    }

    // 9. Return results
    result.insert("status".to_string(), "success".to_string());
    result.insert("rows_written".to_string(), row_count.to_string());
    result.insert("output_path".to_string(), output_path);

    Ok(result)
}

/// Validate feature rows using Rust's high-performance validation engine
///
/// This bridges Python's validation API to Rust for batch validation,
/// providing significant performance improvements for large datasets.
///
/// Args:
///     schema: Schema configuration dict with:
///         - name: Schema name (str)
///         - required_entities: List of required entity names
///         - required_features: List of required feature names
///         - forbidden_features: List of forbidden feature names
///         - type_constraints: Dict of feature_name -> type_name
///         - value_constraints: Dict of feature_name -> list of constraints
///     rows: List of feature row dicts, each with:
///         - entities: List of {name, value} dicts
///         - features: Dict of feature_name -> value
///
/// Returns:
///     List of validation results, one per row:
///         - {"valid": true} for valid rows
///         - {"valid": false, "error": "message"} for invalid rows
#[pyfunction]
fn validate_features_native(
    py: Python,
    schema: &PyDict,
    rows: &PyList,
) -> PyResult<PyObject> {
    // Helper to safely get optional string from dict
    fn get_string(dict: &PyDict, key: &str) -> Option<String> {
        dict.get_item(key).ok().flatten().and_then(|v| v.extract::<String>().ok())
    }

    // Helper to safely get optional list from dict
    fn get_list<'a>(dict: &'a PyDict, key: &str) -> Option<&'a PyList> {
        dict.get_item(key).ok().flatten().and_then(|v| v.downcast::<PyList>().ok())
    }

    // Helper to safely get optional dict from dict
    fn get_dict<'a>(dict: &'a PyDict, key: &str) -> Option<&'a PyDict> {
        dict.get_item(key).ok().flatten().and_then(|v| v.downcast::<PyDict>().ok())
    }

    // 1. Parse schema from Python dict
    let schema_name = get_string(schema, "name").unwrap_or_else(|| "unnamed".to_string());
    let mut rust_schema = FeatureSchema::new(&schema_name);

    // Required entities
    if let Some(entities) = get_list(schema, "required_entities") {
        for entity in entities.iter() {
            if let Ok(name) = entity.extract::<String>() {
                rust_schema = rust_schema.assert_entity(name);
            }
        }
    }

    // Required features
    if let Some(features) = get_list(schema, "required_features") {
        for feature in features.iter() {
            if let Ok(name) = feature.extract::<String>() {
                rust_schema = rust_schema.assert_feature(name);
            }
        }
    }

    // Forbidden features
    if let Some(features) = get_list(schema, "forbidden_features") {
        for feature in features.iter() {
            if let Ok(name) = feature.extract::<String>() {
                rust_schema = rust_schema.reject_feature(name);
            }
        }
    }

    // Type constraints
    if let Some(constraints) = get_dict(schema, "type_constraints") {
        for (key, value) in constraints.iter() {
            if let (Ok(feature_name), Ok(type_name)) = (key.extract::<String>(), value.extract::<String>()) {
                let type_constraint = match type_name.as_str() {
                    "Int" | "int" => TypeConstraint::Int,
                    "Float" | "float" => TypeConstraint::Float,
                    "String" | "str" | "string" => TypeConstraint::String,
                    "Bool" | "bool" => TypeConstraint::Bool,
                    "Json" | "json" => TypeConstraint::Json,
                    "ArrayInt" | "array_int" => TypeConstraint::ArrayInt,
                    "ArrayFloat" | "array_float" => TypeConstraint::ArrayFloat,
                    "ArrayString" | "array_string" => TypeConstraint::ArrayString,
                    "Date" | "date" => TypeConstraint::Date,
                    "Nullable" | "nullable" => TypeConstraint::Nullable,
                    _ => TypeConstraint::Any,
                };
                rust_schema = rust_schema.assert_type(feature_name, type_constraint);
            }
        }
    }

    // Value constraints
    if let Some(constraints) = get_dict(schema, "value_constraints") {
        for (key, value) in constraints.iter() {
            if let Ok(feature_name) = key.extract::<String>() {
                if let Ok(constraint_list) = value.downcast::<PyList>() {
                    for constraint in constraint_list.iter() {
                        if let Ok(constraint_dict) = constraint.downcast::<PyDict>() {
                            if let Some(constraint_type) = get_string(constraint_dict, "type") {
                                let value_constraint = match constraint_type.as_str() {
                                    "IntRange" => {
                                        let min = get_string(constraint_dict, "min")
                                            .and_then(|s| s.parse::<i64>().ok())
                                            .or_else(|| constraint_dict.get_item("min").ok().flatten().and_then(|v| v.extract::<i64>().ok()))
                                            .unwrap_or(i64::MIN);
                                        let max = get_string(constraint_dict, "max")
                                            .and_then(|s| s.parse::<i64>().ok())
                                            .or_else(|| constraint_dict.get_item("max").ok().flatten().and_then(|v| v.extract::<i64>().ok()))
                                            .unwrap_or(i64::MAX);
                                        Some(ValueConstraint::IntRange(min, max))
                                    }
                                    "FloatRange" => {
                                        let min = constraint_dict.get_item("min").ok().flatten()
                                            .and_then(|v| v.extract::<f64>().ok())
                                            .unwrap_or(f64::MIN);
                                        let max = constraint_dict.get_item("max").ok().flatten()
                                            .and_then(|v| v.extract::<f64>().ok())
                                            .unwrap_or(f64::MAX);
                                        Some(ValueConstraint::FloatRange(min, max))
                                    }
                                    "StringEnum" => {
                                        if let Some(values) = get_list(constraint_dict, "values") {
                                            let allowed: Vec<String> = values.iter()
                                                .filter_map(|v| v.extract::<String>().ok())
                                                .collect();
                                            Some(ValueConstraint::StringEnum(allowed))
                                        } else {
                                            None
                                        }
                                    }
                                    "StringLength" => {
                                        let min = constraint_dict.get_item("min").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(0);
                                        let max = constraint_dict.get_item("max").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(usize::MAX);
                                        Some(ValueConstraint::StringLength(min, max))
                                    }
                                    "ArrayLength" => {
                                        let min = constraint_dict.get_item("min").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(0);
                                        let max = constraint_dict.get_item("max").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(usize::MAX);
                                        Some(ValueConstraint::ArrayLength(min, max))
                                    }
                                    _ => None,
                                };
                                if let Some(vc) = value_constraint {
                                    rust_schema = rust_schema.assert_value(feature_name.clone(), vc);
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    // 2. Validate each row
    let mut results = Vec::with_capacity(rows.len());
    let timestamp = chrono::Utc::now();

    for row in rows.iter() {
        let result = if let Ok(row_dict) = row.downcast::<PyDict>() {
            // Parse entities
            let mut entities = Vec::new();
            if let Some(entity_list) = get_list(row_dict, "entities") {
                for entity in entity_list.iter() {
                    if let Ok(entity_dict) = entity.downcast::<PyDict>() {
                        let name = get_string(entity_dict, "name").unwrap_or_default();
                        let value = get_string(entity_dict, "value").unwrap_or_default();
                        entities.push(EntityKey::new(name, value));
                    }
                }
            }

            // Parse features
            let mut features = HashMap::new();
            if let Some(features_dict) = get_dict(row_dict, "features") {
                for (key, value) in features_dict.iter() {
                    if let Ok(feature_name) = key.extract::<String>() {
                        let feature_value = python_to_feature_value(value);
                        features.insert(feature_name, feature_value);
                    }
                }
            }

            let feature_row = FeatureRow {
                entities,
                features,
                timestamp,
            };

            // Validate
            match rust_schema.validate(&feature_row) {
                Ok(()) => ValidationResult { valid: true, error: None },
                Err(e) => ValidationResult { valid: false, error: Some(e.to_string()) },
            }
        } else {
            ValidationResult { valid: false, error: Some("Invalid row format".to_string()) }
        };

        results.push(result);
    }

    // 3. Convert results to Python list
    let py_results = PyList::empty(py);
    for result in results {
        let result_dict = PyDict::new(py);
        result_dict.set_item("valid", result.valid)?;
        if let Some(error) = result.error {
            result_dict.set_item("error", error)?;
        }
        py_results.append(result_dict)?;
    }

    Ok(py_results.into())
}

/// Internal validation result struct
struct ValidationResult {
    valid: bool,
    error: Option<String>,
}

/// Convert Python value to FeatureValue
fn python_to_feature_value(value: &PyAny) -> FeatureValue {
    // Try each type in order of specificity
    if value.is_none() {
        FeatureValue::Null
    } else if let Ok(v) = value.extract::<bool>() {
        FeatureValue::Bool(v)
    } else if let Ok(v) = value.extract::<i64>() {
        FeatureValue::Int(v)
    } else if let Ok(v) = value.extract::<f64>() {
        FeatureValue::Float(v)
    } else if let Ok(v) = value.extract::<String>() {
        FeatureValue::String(v)
    } else if let Ok(list) = value.downcast::<PyList>() {
        // Try to determine array type from first element
        if list.is_empty() {
            FeatureValue::ArrayString(vec![])
        } else if let Ok(first) = list.get_item(0) {
            if first.extract::<i64>().is_ok() {
                let ints: Vec<i64> = list.iter()
                    .filter_map(|v| v.extract::<i64>().ok())
                    .collect();
                FeatureValue::ArrayInt(ints)
            } else if first.extract::<f64>().is_ok() {
                let floats: Vec<f64> = list.iter()
                    .filter_map(|v| v.extract::<f64>().ok())
                    .collect();
                FeatureValue::ArrayFloat(floats)
            } else {
                let strings: Vec<String> = list.iter()
                    .filter_map(|v| v.extract::<String>().ok())
                    .collect();
                FeatureValue::ArrayString(strings)
            }
        } else {
            FeatureValue::ArrayString(vec![])
        }
    } else if let Ok(dict) = value.downcast::<PyDict>() {
        // JSON object - convert to serde_json::Value
        FeatureValue::Json(dict_to_json(dict))
    } else {
        FeatureValue::Null
    }
}

/// Convert Python dict to serde_json::Value
fn dict_to_json(dict: &PyDict) -> serde_json::Value {
    let mut map = serde_json::Map::new();
    for (key, value) in dict.iter() {
        if let Ok(key_str) = key.extract::<String>() {
            let json_value = if value.is_none() {
                serde_json::Value::Null
            } else if let Ok(v) = value.extract::<bool>() {
                serde_json::Value::Bool(v)
            } else if let Ok(v) = value.extract::<i64>() {
                serde_json::Value::Number(v.into())
            } else if let Ok(v) = value.extract::<f64>() {
                serde_json::Value::Number(serde_json::Number::from_f64(v).unwrap_or(0.into()))
            } else if let Ok(v) = value.extract::<String>() {
                serde_json::Value::String(v)
            } else if let Ok(nested_dict) = value.downcast::<PyDict>() {
                dict_to_json(nested_dict)
            } else if let Ok(list) = value.downcast::<PyList>() {
                let arr: Vec<serde_json::Value> = list.iter()
                    .map(|v| {
                        if let Ok(s) = v.extract::<String>() {
                            serde_json::Value::String(s)
                        } else if let Ok(i) = v.extract::<i64>() {
                            serde_json::Value::Number(i.into())
                        } else {
                            serde_json::Value::Null
                        }
                    })
                    .collect();
                serde_json::Value::Array(arr)
            } else {
                serde_json::Value::Null
            };
            map.insert(key_str, json_value);
        }
    }
    serde_json::Value::Object(map)
}

/// Parse a Python dict into a FeatureSchema for validation
///
/// This is used by execute_and_write() to parse the schema before releasing GIL.
/// The schema dict follows the same format as validate_features_native().
fn parse_schema_from_dict(schema: &PyDict) -> PyResult<FeatureSchema> {
    // Helper to safely get optional string from dict
    fn get_string(dict: &PyDict, key: &str) -> Option<String> {
        dict.get_item(key).ok().flatten().and_then(|v| v.extract::<String>().ok())
    }

    // Helper to safely get optional list from dict
    fn get_list<'a>(dict: &'a PyDict, key: &str) -> Option<&'a PyList> {
        dict.get_item(key).ok().flatten().and_then(|v| v.downcast::<PyList>().ok())
    }

    // Helper to safely get optional dict from dict
    fn get_dict<'a>(dict: &'a PyDict, key: &str) -> Option<&'a PyDict> {
        dict.get_item(key).ok().flatten().and_then(|v| v.downcast::<PyDict>().ok())
    }

    let schema_name = get_string(schema, "name").unwrap_or_else(|| "unnamed".to_string());
    let mut rust_schema = FeatureSchema::new(&schema_name);

    // Required entities
    if let Some(entities) = get_list(schema, "required_entities") {
        for entity in entities.iter() {
            if let Ok(name) = entity.extract::<String>() {
                rust_schema = rust_schema.assert_entity(name);
            }
        }
    }

    // Required features
    if let Some(features) = get_list(schema, "required_features") {
        for feature in features.iter() {
            if let Ok(name) = feature.extract::<String>() {
                rust_schema = rust_schema.assert_feature(name);
            }
        }
    }

    // Forbidden features
    if let Some(features) = get_list(schema, "forbidden_features") {
        for feature in features.iter() {
            if let Ok(name) = feature.extract::<String>() {
                rust_schema = rust_schema.reject_feature(name);
            }
        }
    }

    // Type constraints
    if let Some(constraints) = get_dict(schema, "type_constraints") {
        for (key, value) in constraints.iter() {
            if let (Ok(feature_name), Ok(type_name)) = (key.extract::<String>(), value.extract::<String>()) {
                let type_constraint = match type_name.as_str() {
                    "Int" | "int" => TypeConstraint::Int,
                    "Float" | "float" => TypeConstraint::Float,
                    "String" | "str" | "string" => TypeConstraint::String,
                    "Bool" | "bool" => TypeConstraint::Bool,
                    "Json" | "json" => TypeConstraint::Json,
                    "ArrayInt" | "array_int" => TypeConstraint::ArrayInt,
                    "ArrayFloat" | "array_float" => TypeConstraint::ArrayFloat,
                    "ArrayString" | "array_string" => TypeConstraint::ArrayString,
                    "Date" | "date" => TypeConstraint::Date,
                    "Nullable" | "nullable" => TypeConstraint::Nullable,
                    _ => TypeConstraint::Any,
                };
                rust_schema = rust_schema.assert_type(feature_name, type_constraint);
            }
        }
    }

    // Value constraints
    if let Some(constraints) = get_dict(schema, "value_constraints") {
        for (key, value) in constraints.iter() {
            if let Ok(feature_name) = key.extract::<String>() {
                if let Ok(constraint_list) = value.downcast::<PyList>() {
                    for constraint in constraint_list.iter() {
                        if let Ok(constraint_dict) = constraint.downcast::<PyDict>() {
                            if let Some(constraint_type) = get_string(constraint_dict, "type") {
                                let value_constraint = match constraint_type.as_str() {
                                    "IntRange" => {
                                        let min = get_string(constraint_dict, "min")
                                            .and_then(|s| s.parse::<i64>().ok())
                                            .or_else(|| constraint_dict.get_item("min").ok().flatten().and_then(|v| v.extract::<i64>().ok()))
                                            .unwrap_or(i64::MIN);
                                        let max = get_string(constraint_dict, "max")
                                            .and_then(|s| s.parse::<i64>().ok())
                                            .or_else(|| constraint_dict.get_item("max").ok().flatten().and_then(|v| v.extract::<i64>().ok()))
                                            .unwrap_or(i64::MAX);
                                        Some(ValueConstraint::IntRange(min, max))
                                    }
                                    "FloatRange" => {
                                        let min = constraint_dict.get_item("min").ok().flatten()
                                            .and_then(|v| v.extract::<f64>().ok())
                                            .unwrap_or(f64::MIN);
                                        let max = constraint_dict.get_item("max").ok().flatten()
                                            .and_then(|v| v.extract::<f64>().ok())
                                            .unwrap_or(f64::MAX);
                                        Some(ValueConstraint::FloatRange(min, max))
                                    }
                                    "StringEnum" => {
                                        if let Some(values) = get_list(constraint_dict, "values") {
                                            let allowed: Vec<String> = values.iter()
                                                .filter_map(|v| v.extract::<String>().ok())
                                                .collect();
                                            Some(ValueConstraint::StringEnum(allowed))
                                        } else {
                                            None
                                        }
                                    }
                                    "StringLength" => {
                                        let min = constraint_dict.get_item("min").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(0);
                                        let max = constraint_dict.get_item("max").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(usize::MAX);
                                        Some(ValueConstraint::StringLength(min, max))
                                    }
                                    "ArrayLength" => {
                                        let min = constraint_dict.get_item("min").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(0);
                                        let max = constraint_dict.get_item("max").ok().flatten()
                                            .and_then(|v| v.extract::<usize>().ok())
                                            .unwrap_or(usize::MAX);
                                        Some(ValueConstraint::ArrayLength(min, max))
                                    }
                                    _ => None,
                                };
                                if let Some(vc) = value_constraint {
                                    rust_schema = rust_schema.assert_value(feature_name.clone(), vc);
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    Ok(rust_schema)
}

/// Fast schema-level validation against Arrow schema (O(columns), not O(rows))
///
/// This validates:
/// - Forbidden features are not present (PII protection - fail early!)
/// - Required features are present
/// - Required entities are in entity_columns
/// - Type constraints match Arrow DataTypes
///
/// This is MUCH faster than row-by-row validation because it only checks
/// the schema once instead of every row.
fn validate_arrow_schema_fast(
    schema: &FeatureSchema,
    column_names: &[String],
    fields: &arrow::datatypes::Fields,
    entity_columns: &[String],
) -> PyResult<()> {
    use arrow::datatypes::DataType;

    // 1. Check forbidden features - FAIL EARLY (PII protection)
    // This is the most important check - prevents PII from being written
    for forbidden in &schema.forbidden_features {
        if column_names.contains(forbidden) {
            return Err(PyRuntimeError::new_err(format!(
                "Schema '{}': Forbidden feature '{}' found in query output. \
                 Remove this column from SELECT to comply with data policy.",
                schema.name, forbidden
            )));
        }
    }

    // 2. Check required features
    for required in &schema.required_features {
        if !column_names.contains(required) {
            return Err(PyRuntimeError::new_err(format!(
                "Schema '{}': Required feature '{}' not found in query output",
                schema.name, required
            )));
        }
    }

    // 3. Check required entities
    for required_entity in &schema.required_entities {
        if !entity_columns.contains(required_entity) {
            return Err(PyRuntimeError::new_err(format!(
                "Schema '{}': Required entity '{}' not found in entity_columns",
                schema.name, required_entity
            )));
        }
    }

    // 4. Check type constraints against Arrow DataTypes
    for (feature_name, constraint) in &schema.type_constraints {
        if let Some(field) = fields.iter().find(|f| f.name() == feature_name) {
            let arrow_type = field.data_type();
            let matches = match (constraint, arrow_type) {
                (TypeConstraint::Int, DataType::Int32 | DataType::Int64) => true,
                (TypeConstraint::Float, DataType::Float32 | DataType::Float64) => true,
                (TypeConstraint::String, DataType::Utf8 | DataType::LargeUtf8) => true,
                (TypeConstraint::Bool, DataType::Boolean) => true,
                (TypeConstraint::Date, DataType::Date32 | DataType::Date64) => true,
                (TypeConstraint::Any, _) => true,
                (TypeConstraint::Nullable, _) => true, // All Arrow types can be null
                // For Float type, also accept Decimal (common in SQL aggregations)
                (TypeConstraint::Float, DataType::Decimal128(_, _)) => true,
                // Arrays - check inner type
                (TypeConstraint::ArrayInt, DataType::List(f)) => {
                    matches!(f.data_type(), DataType::Int32 | DataType::Int64)
                }
                (TypeConstraint::ArrayFloat, DataType::List(f)) => {
                    matches!(f.data_type(), DataType::Float32 | DataType::Float64)
                }
                (TypeConstraint::ArrayString, DataType::List(f)) => {
                    matches!(f.data_type(), DataType::Utf8 | DataType::LargeUtf8)
                }
                // JSON is typically stored as String in Arrow
                (TypeConstraint::Json, DataType::Utf8 | DataType::LargeUtf8) => true,
                _ => false,
            };

            if !matches {
                return Err(PyRuntimeError::new_err(format!(
                    "Schema '{}': Feature '{}' type mismatch. Expected {:?}, got {:?}",
                    schema.name, feature_name, constraint, arrow_type
                )));
            }
        }
    }

    Ok(())
}

#[pymodule]
fn featureduck_native(_py: Python, m: &PyModule) -> PyResult<()> {
    // Production API: SQLFrame → SQL → execute_and_write → Delta Lake
    m.add_function(wrap_pyfunction!(execute_and_write, m)?)?;

    // Validation bridge: High-performance Rust validation for batch processing
    m.add_function(wrap_pyfunction!(validate_features_native, m)?)?;

    // REMOVED (LogicalPlan deprecated):
    // - compile_to_duckdb_sql() - Used LogicalPlan, replaced by SQLFrame
    // - PyDeltaConnector - Used LogicalPlan, replaced by execute_and_write()

    Ok(())
}
